{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4abdf62-4372-42ed-be93-0645d2b84e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.16.2\n",
      "Devices Available: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Devices Available:\", tf.config.list_physical_devices())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ee7508-0797-472b-beff-6b40fecaa3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:42:40.671331: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-31 17:42:40.672480: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 - 5s - 62ms/step - accuracy: 0.6102 - loss: 0.6386 - val_accuracy: 0.7932 - val_loss: 0.4460\n",
      "Epoch 2/10\n",
      "79/79 - 4s - 48ms/step - accuracy: 0.8297 - loss: 0.3824 - val_accuracy: 0.8476 - val_loss: 0.3430\n",
      "Epoch 3/10\n",
      "79/79 - 3s - 43ms/step - accuracy: 0.9071 - loss: 0.2382 - val_accuracy: 0.8554 - val_loss: 0.3616\n",
      "Epoch 4/10\n",
      "79/79 - 3s - 39ms/step - accuracy: 0.9477 - loss: 0.1508 - val_accuracy: 0.8406 - val_loss: 0.4459\n",
      "Epoch 5/10\n",
      "79/79 - 3s - 40ms/step - accuracy: 0.9500 - loss: 0.1310 - val_accuracy: 0.8560 - val_loss: 0.4116\n",
      "Epoch 6/10\n",
      "79/79 - 3s - 40ms/step - accuracy: 0.9801 - loss: 0.0649 - val_accuracy: 0.8374 - val_loss: 0.5219\n",
      "Epoch 7/10\n",
      "79/79 - 4s - 50ms/step - accuracy: 0.9958 - loss: 0.0222 - val_accuracy: 0.8450 - val_loss: 0.5627\n",
      "Epoch 8/10\n",
      "79/79 - 4s - 55ms/step - accuracy: 0.9956 - loss: 0.0179 - val_accuracy: 0.8446 - val_loss: 0.6431\n",
      "Epoch 9/10\n",
      "79/79 - 4s - 57ms/step - accuracy: 0.9972 - loss: 0.0122 - val_accuracy: 0.8454 - val_loss: 0.6991\n",
      "Epoch 10/10\n",
      "79/79 - 5s - 57ms/step - accuracy: 0.9996 - loss: 0.0039 - val_accuracy: 0.8492 - val_loss: 0.7146\n",
      "Training completed in 0.65 minutes\n",
      "98/98 - 2s - 19ms/step - accuracy: 0.8385 - loss: 0.7567\n",
      "Test Loss: 0.7567\n",
      "Test Accuracy: 0.8385\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step  \n",
      "Review: [  49   23   90   12  102 9069   18  827 3503   10   14  278    5    2\n",
      "  759  388   18 3162 2136    1   18  556    1 8604    1  642    6   12\n",
      " 1141    1    1  404  539    1 5900 2481    4   15    1    4   15 2177\n",
      "   15  101    5    2 3348   98    9  633 1405   11  298  227    9  205\n",
      "    2  376 5577    4   60   89    9   86   96    9 1405  166   11  298\n",
      "   38    6 1231    1  843    7 3292    1    7   38    1  376    5 2502\n",
      "    1   12   28   61   27    6  690  190   17    3  889    5   11  258\n",
      "    4   75]...\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "---\n",
      "Review: [   3    1  654  643    5    3    1 3059    1    1    2    1   12    1\n",
      "    1   14  514    6 3350    8  130  514    6  401    3 1521    1    1\n",
      "   17    3 5591    5    1   15   35    1   39   25 7698 1342    5   20\n",
      "  283   10   14   35 7998  843    6    1   17  122  315    4  468 4070\n",
      "    1    1  203 6797  105    5    1  850 7698   90    8 1369    5    2\n",
      "  111 8371    1    7  269 1709    4    1   13    2  776 4491    7  144\n",
      "   12   14 4615  170    8    1   94  257  159  290    8 4131   10 2062\n",
      "    2 2006]...\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "---\n",
      "Review: [ 649   19 2918 2260   19 1522   19  716    2    1   24  176 1460   19\n",
      "    4  176  743   19  283 2632   19    2    1    8    3  199   12  707\n",
      "    1 2676    2 5479  508  149  452  794   10    9   27    3 2135 1793\n",
      "   18 2873  143   15 6365    4    2 1089 1199   16   22  119 1022    1\n",
      "    3 4585   38   83   51   22   65   69    2 2800    3 3623  133    8\n",
      "  185    2   71  142   12  219   66    1   22   82  153    6   69   11\n",
      " 3261    7    2  787  174   16 2204    1 5664    1   13   22   65  401\n",
      "   29  878]...\n",
      "Actual Sentiment: Negative\n",
      "Predicted Sentiment: Negative\n",
      "---\n",
      "Review: [ 352 5238    1  437  298    6 4430   47    1    6   11    1    1 8454\n",
      "   19   16  418    1   34    2    1 7038    5    3   20  171   24    2\n",
      "  210  169    1    8    1   13   11   14  495   34 3837 5818  114   29\n",
      "   27    3 4420  227   91  238  789   39   95 5818   65   75 1262  441\n",
      "    1   17  260   15    1   34  566 1017   38    9   86  119   48  619\n",
      "    6   95  140   11    7    2  253    5   19   12  148  160    6 1923\n",
      "   12  125    7 5600    4 8341    4   12 4354 6715 8575  203 4509   33\n",
      "  219   26]...\n",
      "Actual Sentiment: Negative\n",
      "Predicted Sentiment: Negative\n",
      "---\n",
      "Review: [  15    3 1099    1 1299    9  509   11   20   52   77   10   14   38\n",
      "  849    4  174   10   66 1481   57    2  936   12 2487  838    1 1567\n",
      "    9  375  105   81 1567   78  744   11  696   92  203    9  566   43\n",
      "   11   19    7   31   12  222    6  106   18   88   17   54 2005  696\n",
      "   16   30  210   28    5   55  354   36   45  110  678   21    1   79\n",
      "  509   10   52   77    9  182   10 1548   46    5  321    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]...\n",
      "Actual Sentiment: Positive\n",
      "Predicted Sentiment: Positive\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:43:21.711017: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Load and Preprocess the IMDb Dataset\n",
    "# ---------------------------------------------------------------\n",
    "# Load the IMDb dataset and split into training, validation, and test sets\n",
    "train_data, val_data, test_data = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train[:80%]', 'train[80%:]', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=False\n",
    ")\n",
    "\n",
    "# Constants\n",
    "MAX_FEATURES = 10000  # Vocabulary size\n",
    "MAX_LEN = 200  # Maximum sequence length\n",
    "BATCH_SIZE = 256  # Batch size for better GPU utilization\n",
    "\n",
    "# TextVectorization Layer (on CPU)\n",
    "with tf.device('/CPU:0'):\n",
    "    vectorizer = TextVectorization(max_tokens=MAX_FEATURES, output_sequence_length=MAX_LEN)\n",
    "    text_ds = train_data.map(lambda x, y: x).take(1000).cache()  # Use a subset to adapt the vocabulary\n",
    "    vectorizer.adapt(text_ds)\n",
    "\n",
    "# Preprocessing function for datasets\n",
    "def preprocess_text(text, label):\n",
    "    return vectorizer(text), tf.cast(label, tf.int32)\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset = (\n",
    "    train_data\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    val_data\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    test_data\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Build the Conv1D-Based Model\n",
    "# ---------------------------------------------------------------\n",
    "# Enable mixed precision for faster training on Apple GPUs\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Define the Conv1D model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(MAX_LEN,)),  # Explicit input shape\n",
    "    tf.keras.layers.Embedding(MAX_FEATURES, 128),  # Embedding layer\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),  # Global pooling to reduce dimensions\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout for regularization\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')  # Output layer in float32 for stability\n",
    "])\n",
    "\n",
    "# Compile the model with mixed precision optimizer\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Train the Model with Validation Set\n",
    "# ---------------------------------------------------------------\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time / 60:.2f} minutes\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Evaluate the Model on Test Data\n",
    "# ---------------------------------------------------------------\n",
    "results = model.evaluate(test_dataset, verbose=2)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Make Predictions on Test Data\n",
    "# ---------------------------------------------------------------\n",
    "for text_batch, label_batch in test_dataset.take(1):  # Take one batch of test data\n",
    "    predictions = model.predict(text_batch)  # Predict sentiment scores (probabilities)\n",
    "    \n",
    "    predicted_labels = tf.where(predictions > 0.5, 1, 0)  # Convert probabilities to binary labels (0 or 1)\n",
    "\n",
    "    # Print a few examples with their predictions\n",
    "    for i in range(5):  # Display first 5 samples in the batch\n",
    "        print(f\"Review: {text_batch[i].numpy()[:100]}...\")  # Show part of the review (vectorized form)\n",
    "        print(f\"Actual Sentiment: {'Positive' if label_batch[i].numpy() == 1 else 'Negative'}\")\n",
    "        print(f\"Predicted Sentiment: {'Positive' if predicted_labels[i].numpy() == 1 else 'Negative'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebc0b0-d873-42ba-a413-55593537f669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
